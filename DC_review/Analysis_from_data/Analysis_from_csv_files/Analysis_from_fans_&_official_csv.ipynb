{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O686gHuu8ZfV",
        "outputId": "ba0aee5a-79ff-450c-9523-e544defb3426"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# FINAL SCRIPT WITH HIGH-ENGAGEMENT ANALYSIS (CORRECTED)\n",
        "# ==============================================================================\n",
        "\n",
        "# STEP 1: SETUP\n",
        "print(\"STEP 1: Setting up the environment...\")\n",
        "!pip install wordcloud -q\n",
        "import nltk\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "print(\"✅ Environment setup is complete.\\n\")\n",
        "\n",
        "\n",
        "# STEP 2: MAIN ANALYSIS\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# --- Loading Your Data from Colab's Session Storage ---\n",
        "print(\"STEP 2: Loading data...\")\n",
        "try:\n",
        "    # Using your specified filenames\n",
        "    fans_df = pd.read_csv('your_fans_data.csv')\n",
        "    official_df = pd.read_csv('your_official_data.csv')\n",
        "    print(\"✅ Data loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ FATAL ERROR: Files not found! Please re-upload your CSV files.\")\n",
        "    raise\n",
        "\n",
        "# --- NLP & Data Preprocessing Pipeline ---\n",
        "\n",
        "# 1. Likes Conversion\n",
        "def convert_likes_to_int(like_value):\n",
        "    if pd.isna(like_value): return 0\n",
        "    if isinstance(like_value, str):\n",
        "        cleaned_string = like_value.replace(',', '').strip()\n",
        "        if not cleaned_string: return 0\n",
        "        try: return int(float(cleaned_string))\n",
        "        except (ValueError, TypeError): return 0\n",
        "    try: return int(like_value)\n",
        "    except (ValueError, TypeError): return 0\n",
        "\n",
        "print(\"\\nSTEP 3: Preprocessing data...\")\n",
        "fans_df['likes_int'] = fans_df['likes'].apply(convert_likes_to_int)\n",
        "official_df['likes_int'] = official_df['likes'].apply(convert_likes_to_int)\n",
        "print(\" -> 'likes' column converted to integers.\")\n",
        "\n",
        "# 2. Text Preprocessing Setup\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "english_stop_words = set(stopwords.words('english'))\n",
        "custom_stop_words = {'delhi', 'capitals', 'dc', 'ipl', 'hai', 'nayi', 'dilli', 'yeh', 'match', 'game', 'vs', 'year', 'go', 'let', 'come', 'one'}\n",
        "hinglish_stop_words = {'aur', 'bhi', 'bhai', 'bahut', 'chalo', 'gaya', 'ho', 'hua', 'iss', 'ka', 'ke', 'ki', 'ko', 'kya', 'mein', 'se', 'tha', 'the', 'toh', 'kar', 'jeetna', 'saal', 'bura', 'laga', 'jeetenge'}\n",
        "stop_words = english_stop_words.union(custom_stop_words).union(hinglish_stop_words)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
        "    return \" \".join(processed_tokens)\n",
        "\n",
        "# Apply text cleaning\n",
        "# This handles the different structures of your two files correctly.\n",
        "fans_df['full_text'] = fans_df['caption'].fillna('') + ' ' + fans_df['hashtags'].fillna('')\n",
        "# This part correctly handles that 'official_df' has no 'hashtags' column.\n",
        "official_df['full_text'] = official_df['caption'].fillna('')\n",
        "\n",
        "fans_df['cleaned_text'] = fans_df['full_text'].apply(preprocess_text)\n",
        "official_df['cleaned_text'] = official_df['full_text'].apply(preprocess_text)\n",
        "print(\" -> Text preprocessing complete.\")\n",
        "print(\"✅ Preprocessing finished successfully.\")\n",
        "\n",
        "# --- Helper Functions for Analysis & Visualization ---\n",
        "def get_top_n_words(corpus, n=None):\n",
        "    if corpus.empty or corpus.str.strip().eq('').all(): return []\n",
        "    vec = CountVectorizer().fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "def plot_frequent_words(top_words, title, color='skyblue'):\n",
        "    if not top_words:\n",
        "        print(f\"Skipping plot for '{title}' as there are no words to display.\")\n",
        "        return\n",
        "    df = pd.DataFrame(top_words, columns=['Word', 'Frequency'])\n",
        "    # Making plot taller to accommodate 50 words\n",
        "    plt.figure(figsize=(12, 15))\n",
        "    sns.barplot(x='Frequency', y='Word', data=df, palette=[color])\n",
        "    plt.title(title, fontsize=16); plt.xlabel('Frequency', fontsize=12); plt.ylabel('Word', fontsize=12); plt.show()\n",
        "\n",
        "def plot_word_cloud(corpus, title):\n",
        "    all_text = \" \".join(corpus)\n",
        "    if not all_text.strip():\n",
        "        print(f\"Cannot generate word cloud for '{title}' because there are no words to display.\")\n",
        "        return\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(all_text)\n",
        "    plt.figure(figsize=(15, 8)); plt.imshow(wordcloud, interpolation='bilinear'); plt.axis('off'); plt.title(title, fontsize=16); plt.show()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# NEW SECTION: ANALYSIS OF HIGH-ENGAGEMENT POSTS (TOP 50% BY LIKES)\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANALYSIS OF HIGH-ENGAGEMENT POSTS (TOP 50% BY LIKES)\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# --- Analysis for Fans' High-Engagement Posts ---\n",
        "fan_likes_median = fans_df['likes_int'].quantile(0.5)\n",
        "print(f\"Fan Posts Median Likes (Top 50% Threshold): {fan_likes_median:,.0f}\")\n",
        "top_50_percent_fans_df = fans_df[fans_df['likes_int'] >= fan_likes_median].copy()\n",
        "\n",
        "if not top_50_percent_fans_df.empty:\n",
        "    # <<< FIX #1: Changed from 20 to 50 to match your request >>>\n",
        "    top_50_fan_words = get_top_n_words(top_50_percent_fans_df['cleaned_text'], 50)\n",
        "\n",
        "    # <<< FIX #2: Changed variable name from top_20_fan_words to top_50_fan_words >>>\n",
        "    # <<< FIX #3: Changed plot title from \"Top 20\" to \"Top 50\" >>>\n",
        "    plot_frequent_words(\n",
        "        top_50_fan_words,\n",
        "        \"Top 50 Words from High-Engagement Fan Posts (Top 50% by Likes)\",\n",
        "        color='#004C97'\n",
        "    )\n",
        "    plot_word_cloud(\n",
        "        top_50_percent_fans_df['cleaned_text'],\n",
        "        \"Word Cloud from High-Engagement Fan Posts\"\n",
        "    )\n",
        "else:\n",
        "    print(\"No fan posts found in the top 50% by likes to analyze.\")\n",
        "\n",
        "\n",
        "# --- Analysis for Official Account's High-Engagement Posts ---\n",
        "print(\"-\" * 60)\n",
        "official_likes_median = official_df['likes_int'].quantile(0.5)\n",
        "print(f\"Official Account Median Likes (Top 50% Threshold): {official_likes_median:,.0f}\")\n",
        "top_50_percent_official_df = official_df[official_df['likes_int'] >= official_likes_median].copy()\n",
        "\n",
        "if not top_50_percent_official_df.empty:\n",
        "    # <<< FIX #1: Changed from 20 to 50 to match your request >>>\n",
        "    top_50_official_words = get_top_n_words(top_50_percent_official_df['cleaned_text'], 50)\n",
        "\n",
        "    # <<< FIX #2: Changed variable name from top_20_official_words to top_50_official_words >>>\n",
        "    # <<< FIX #3: Changed plot title from \"Top 20\" to \"Top 50\" >>>\n",
        "    plot_frequent_words(\n",
        "        top_50_official_words,\n",
        "        \"Top 50 Words from High-Engagement Official Posts (Top 50% by Likes)\",\n",
        "        color='#EF1E23'\n",
        "    )\n",
        "    plot_word_cloud(\n",
        "        top_50_percent_official_df['cleaned_text'],\n",
        "        \"Word Cloud from High-Engagement Official Posts\"\n",
        "    )\n",
        "else:\n",
        "    print(\"No official posts found in the top 50% by likes to analyze.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# ORIGINAL OVERALL ANALYSIS (Still useful for comparison)\n",
        "# This part is left unchanged to show Top 20 overall words.\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OVERALL ANALYSIS (ALL POSTS)\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "top_fan_words_overall = get_top_n_words(fans_df['cleaned_text'], 20)\n",
        "plot_frequent_words(top_fan_words_overall, \"Top 20 Words from ALL Fan Posts\", color='#004C97')\n",
        "\n",
        "top_official_words_overall = get_top_n_words(official_df['cleaned_text'], 20)\n",
        "plot_frequent_words(top_official_words_overall, \"Top 20 Words from ALL Official Posts\", color='#EF1E23')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
